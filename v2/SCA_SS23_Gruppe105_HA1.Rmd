---
title: "SCA_SS23_Gruppe105_HA1"
author: "Cordelia Mena Hernandez"
date: "2023-05-09"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```



## Aufgabe 1

```{r}
cost <- read.csv("data/output_cost_8Players_v0020.csv", sep=";", dec=",")
services <- read.csv("data/output_services_8Players_v0020.csv", sep=";", dec=",")
prices <- read.csv("data/output_prices_8Players_v0020.csv", sep=";", dec=",")
transactions <- read.csv("data/output_transactions_8Players_v0020.csv", sep=";", dec=",")

```


```{r}
summary(cost)
```

```{r}
summary(services)
```

```{r}
summary(prices)
```


```{r}
summary(transactions)
```


### Aufgabe 2


```{r}
library(lubridate)
cost$Date <- make_date(cost$Year, cost$Month)
cost <- subset(cost, select=-c(Year, Month))

services$Date <- make_date(services$Year, services$Month, services$Day)
services <- subset(services, select=-c(Year, Month, Day))

transactions$Date <- make_date(transactions$Year, transactions$Month, transactions$Day)
transactions <- subset(transactions, select=-c(Year, Month, Day))
```

```{r}
year18_22 <- interval(ymd("2018-01-01"), ymd("2022-12-31"))

cost18_22 <- cost[cost$Date %within% year18_22, ]
services18_22 <- services[services$Date %within% year18_22, ]
transactions18_22 <- transactions[transactions$Date %within% year18_22, ]
```



```{r}
summary(cost18_22)
```

```{r}
summary(services18_22)
```
```{r}
summary(prices)
```

```{r}
summary(transactions18_22)

```

## Aufgabe 3
```{r}
transactions_regions <- data.frame(regions=unique(transactions18_22$region))
print(transactions_regions)
```
## Aufgabe 4

```{r}
service_vendors <- services18_22[, c("vendor", "service")]
service_vendors <- unique(service_vendors)
service_vendors <- service_vendors[order(service_vendors$service),]
print(service_vendors)
```

## Aufgabe 5
```{r}
supermarkets <- transactions18_22[, c("region", "storename")]
supermarkets <- unique(supermarkets)
supermarkets <- subset(supermarkets, region %in% c('Shangh','Peking', 'Skorea'))

print(supermarkets)
```
### Marktübersicht

## Augabe 6

```{r}
totalsales <- aggregate(transactions18_22$Sales, by=list(transactions18_22$Product), FUN=sum)
print(totalsales)
```

```{r}
totalsales_105 <- sum(transactions18_22[transactions18_22$Product =="Gruppe105",4])
print(totalsales_105)
```

```{r}
total_marketsales <-sum(totalsales[totalsales$Group.1 != "Lost Sales",2])
marketshare <- totalsales_105/total_marketsales
print(marketshare)
```
 Das verkaufte Produkt Gruppe105 hat 12,41% der gesamten Marktanteils
 
## Aufgabe 7
```{r}
regions_105 <- subset(transactions18_22, Product == 'Gruppe105',select = c('region','storename','Sales'))
```

```{r}
regions_summary <- aggregate(regions_105$Sales, by=list(regions_105$region), FUN=sum)
colnames(regions_summary) <- c('Region', 'Total sales')

```

```{r}
regions_summary$supermrkt_average <- regions_summary$`Total sales`/5
```


```{r}
min(regions_summary$supermrkt_average)/max(regions_summary$supermrkt_average)
```

Es besteht 65,42% Unterschied zwischen der absatzstärksten und der absatzschwächsten Region.


## Aufgabe 8

```{r}
yearly_marketshare <- transactions18_22 %>% filter(Product != 'Lost Sales') %>% group_by(year = floor_date(Date, 'year'), Product) %>% summarise(sum_sales= sum(Sales))

marketshare105 <- yearly_marketshare %>% mutate(totalsales_year = sum(sum_sales)) %>% filter(Product=='Gruppe105') %>%  mutate(relative_marketshare = sum_sales/totalsales_year)

marketshare105

```
Der Marktanteil des Produkts 105 bleibt relativ konstant bei ungefähr 12%, dies ändert sich fast nicht über die Jahren.

## Aufgabe 9

```{r}
monthlymean_sales <- transactions18_22 %>% filter(Product == 'Gruppe105') %>% group_by(month = floor_date(Date, 'month')) %>% summarise(mean_sales= mean(Sales))

monthlymean_sales[which.max(monthlymean_sales$mean_sales),]
```
Der Monat mit den höchsten durchschnittlichen Absatz von 18 Stück ist Oktober 2019.

## Aufgabe 10
```{r}
revenue_105 <- totalsales_105*3.9
revenue_105
```


```{r}
totalcost_105 <- sum(cost18_22[cost18_22$Product =="Gruppe105",2])
print(totalcost_105)
```

Die Gesamtkosten betragen 1946187 Eur.

```{r}
profit_105 <- revenue_105 - totalcost_105
profit_105
```

## Aufgabe 11

```{r}
rev_data105 <- transactions18_22[, c("Product", "Sales", "Date")]
rev_data105 <- subset(rev_data105, Product %in% c('Gruppe105'))
rev_data105 <- aggregate(rev_data105$Sales, by=list(floor_date(rev_data105$Date, 'month')), FUN=sum)
rev_data105$x <- rev_data105$x*3.9

cost_data105 <- subset(cost18_22, Product %in% c('Gruppe105'))
cost_data105 <- aggregate(cost_data105$Amount, by=list(floor_date(cost_data105$Date, 'month')), FUN=sum)
```

```{r}
profit_data105 <- merge(x = rev_data105, y = cost_data105, by = "Group.1", all = TRUE)
colnames(profit_data105) <- c('Date', 'Revenue', 'Cost')
profit_data105$Profit <- profit_data105$Revenue - profit_data105$Cost
```

```{r}
library(ggplot2)
ggplot(data=profit_data105, aes(x=Date, y=Profit)) +
  geom_line(color="blue")+
  geom_point()+
  geom_point(data = profit_data105[which.min(profit_data105$Profit), ], color="red", size=2) +
  geom_point(data = profit_data105[which.max(profit_data105$Profit), ], color="orange", size=2)+
  scale_x_date(date_labels = "%Y %b")+
  scale_x_date(date_breaks = "3 month", date_labels = "%b %Y")+
  theme(axis.text.x=element_text(angle=60, hjust=1))
```
Mai 2019 erweist den kleinsten Profit und Januar 2018 den größten.

## Aufgabe 12

```{r}
shipping_data105 <- services18_22[, c("region", "Product", "DaysScheduled", "DaysExecuted","cost")]
shipping_data105 <- subset(shipping_data105, Product %in% c('Gruppe105'))

shipping_data105$late <- ifelse(shipping_data105$DaysExecuted > shipping_data105$DaysScheduled, TRUE, FALSE)

shipping_byregion <- summarise(group_by(shipping_data105, region, late), total_cost=sum(cost))

```


```{r}

library(scales)
ggplot(data=shipping_byregion, aes(x=region, y=total_cost, fill=late)) +
  geom_col()+
  xlab("Region") + 
  ylab("Cost")+
  scale_y_continuous(labels = unit_format(unit = "K", scale = 1e-3))

```

## Aufgabe 13

```{r}
warehouse_data <- services18_22 %>% filter(Product=="Gruppe105", service=="Warehousing" ) %>% summarise(sum_scheduled= sum(QScheduled), sum_exec=sum(QExecuted), sum_cost=sum(cost))
real_unitcost <- warehouse_data$sum_cost/warehouse_data$sum_exec
real_unitcost/0.6-1

```
Die Kosten für die tatsaechliche Lagerleistung beträgt 1,57 Geldeinheiten pro Stück. Das ist 162,32% mehr als die 0,6 Geldeinheiten pro Stück, die vereinbart waren.

## Aufgabe 14

Wir verwenden das Produkt von On-Time Delivery (OTD) und In-Full Rate (IFR) zur Bewertung der Transportdiensleister.
$$
KPI = IFR * OTD = \frac{\#\ of\ orders\ delivered\ complete}{\#\ of\ customer\ orders} * \frac{\#\ of\ orders\ delivered\ on\ or\ before\ due\ date}{\#\ of\ customer\ orders}
$$
Beide zusammengenommen beschreiben, wie gut wir in der Lage sind, Kundenaufträge zu erfüllen.
Wir haben uns bewusst gegen OTIF (On-Time, In-Full) als KPI entschieden, die nur Lieferungen im Nenner hätte, die sowohl vollständig als auch pünktlich sind. Die Annahme dahinter ist, dass wir auch unvollständige Lieferungen verkaufen können (wo nur einzelne Flaschen fehlen), nur eben weniger Flaschen.

```{r}
# Add new column for IFR and On-time delivery
services18_22$IFR <- services18_22$QExecuted / services18_22$QScheduled
services18_22$On_Time <- services18_22$DaysExecuted <= services18_22$DaysScheduled  # true/false

# Construct table of our vendors with quality KPI using dplyr pipe syntax
shipping_vendor_quality <- services18_22 %>%
  # Filter to only use our group's data and only look at shipping services 
  filter(Product=="Gruppe105", service=="Shipping" ) %>%
  
  # Group by Vendors
  group_by(vendor) %>%
  
  # Calculate our Vendor Quality KPI (as described above)
  # Also calculate the OTD (using the mean is sufficient because the column is TRUE (1) for on-time deliveries and FALSE (0) for late deliveries)
  # Also calculate average IFR, average cost per executed quentity and total number of shipments by that customer
 summarise(Vendor_Quality=mean(On_Time)*mean(IFR),
            OTD=mean(On_Time),
            IFR=mean(IFR),
            Avg_Cost_per_Qty=mean(cost/QExecuted),
            No_Shipments=n()) %>%  
  
  # Sort by Vendor Quality KPI
  arrange(Vendor_Quality)

shipping_vendor_quality
```

Der schlechteste Shipping-Dienstleister nach unserer Qualitätskennzahl ist hier JNT Shipping, die eine deutlich schlechtere Liefertreue bzw. OTD haben als die anderen Dienstleister. Da die IFR für Shipping-Dienstleistungen in diesem Datensatz immer 1 beträgt, ist dies der einzige Einflussfaktor. Der beste Shipping-Dienstleister, Bange+Hammer Shipping, führt für uns 690 Lieferungen aus. Bei JNT Shipping sind es nur unwesentlich weniger (660).
Neben der reinen Qualität sollten die Dienstleister auch nach ihren Kosten bewertet werden (siehe Spalte "Avg_Cost_per_Qty", also Kosten pro tatsächlich ausgeführter Einheit).

## Aufgabe 15

Wir verwenden hier analog und mit der gleichen Begründung wie bei den Transportdienstleistern eine kombinierte Rate aus OTD und IFR.

```{r}
# Columns for IFR and On-time delivery --> already added above

# Construct table of our vendors with quality KPI using dplyr pipe syntax
# Analogous to above
warehouse_vendor_quality <- services18_22 %>%
  # Filter to only use our group's data and only look at Warehouse services 
  filter(Product=="Gruppe105", service=="Warehousing" ) %>%
  
  # Group by Vendors
  group_by(vendor) %>%
  
  # Calculate our Vendor Quality KPI (as described above)
  # Also calculate the OTD (using the mean is sufficient because the column is TRUE (1) for on-time deliveries and FALSE (0) for late deliveries)
  # Also calculate average IFR, average cost per executed quentity and total number of services by that customer
 summarise(Vendor_Quality=mean(On_Time)*mean(IFR),
            OTD=mean(On_Time),
            IFR=mean(IFR),
            Avg_Cost_per_Qty=mean(cost/QExecuted),
            No_WH_Services=n()) %>%  
  
  # Sort by Vendor Quality KPI
  arrange(desc(Vendor_Quality))

warehouse_vendor_quality
```

Der schlechteste Warehouse-Dienstleister nach unserer Qualitätskennzahl ist hier IntEx Warehousing, die die schlechsteste IFR Da die IFR für Shipping-Dienstleistungen in diesem Datensatz immer 1 beträgt, ist dies der einzige Einflussfaktor. Insgesamt liegen die Dienstleister jedoch sehr nah beieinander (weniger als 2 Prozentpunkte Unterschied zwischen dem besten und schlechtesten). Auch die Anzahl der erbrachten Services ("No_WH_Services") ist sehr ähnlich.
Neben der reinen Qualität sollten die Dienstleister auch nach ihren Kosten bewertet werden (siehe Spalte "Avg_Cost_per_Qty", also Kosten pro tatsächlich ausgeführter Einheit).


## Aufgabe 16
```{r}
ggplot(data=warehouse_vendor_quality, aes(x=reorder(vendor, Vendor_Quality, decreasing=TRUE), y=Vendor_Quality)) +
  geom_col() +
  geom_point(aes(x=reorder(vendor, Vendor_Quality, decreasing=TRUE), y=Avg_Cost_per_Qty-1, color="Cost")) +
  xlab("Warehouse-Dienstleister") + 
  ylab("Quality") +
  theme(legend.title=element_blank()) +
  theme(axis.text.x=element_text(angle=60, hjust=1)) +
  scale_y_continuous(name="Quality", sec.axis = sec_axis(~.+1, name="Cost/Executed Qty"))
```
Insgesamt lassen sich bei den Warehouse-Dienstleistern nur wenig Unterschiede erkennen, weder bei der Qualität noch den Kosten. Eine Verbesserung der Performance scheint möglich (siehe z. B. Flying Mercury Warehousing mit vergleichsweise hoher Qualität bei niedrigen Kosten), wird aber vermutlich recht klein ausfallen. 

## Aufgabe 17
```{r}
ggplot(data=shipping_vendor_quality, aes(x=reorder(vendor, Vendor_Quality, decreasing=TRUE), y=Vendor_Quality)) +
  geom_col() +
  geom_point(aes(x=reorder(vendor, Vendor_Quality, decreasing=TRUE), y=Avg_Cost_per_Qty-1, color="Cost")) +
  xlab("Shipping-Dienstleister") + 
  ylab("Quality") +
  theme(legend.title=element_blank()) +
  theme(axis.text.x=element_text(angle=60, hjust=1)) +
  scale_y_continuous(name="Quality", sec.axis = sec_axis(~.+1, name="Cost/Executed Qty"))
```
Anders als bei den Warehousing-Dienstleistern lassen sich hier große Unterschiede in der Qualität feststellen. Bange+Hammer Shipping sticht durch eine deutlich höhere Qualität hervor (getrieben durch eine hohe OTD Rate).