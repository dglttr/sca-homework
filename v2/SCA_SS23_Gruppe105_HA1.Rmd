---
title: "SCA_SS23_Gruppe105_HA1"
author: "Cordelia Mena Hernandez, Daniel Glatter"
date: "2023-05-09"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(ggplot2)
```

# Übersicht

## Aufgabe 1

```{r}
cost <- read.csv("data/output_cost_8Players_v0020.csv", sep=";", dec=",")
services <- read.csv("data/output_services_8Players_v0020.csv", sep=";", dec=",")
prices <- read.csv("data/output_prices_8Players_v0020.csv", sep=";", dec=",")
transactions <- read.csv("data/output_transactions_8Players_v0020.csv", sep=";", dec=",")

```


```{r}
summary(cost)
```

```{r}
summary(services)
```

```{r}
summary(prices)
```


```{r}
summary(transactions)
```


## Aufgabe 2


```{r}
library(lubridate)
cost$Date <- make_date(cost$Year, cost$Month)
cost <- subset(cost, select=-c(Year, Month))

services$Date <- make_date(services$Year, services$Month, services$Day)
services <- subset(services, select=-c(Year, Month, Day))

transactions$Date <- make_date(transactions$Year, transactions$Month, transactions$Day)
transactions <- subset(transactions, select=-c(Year, Month, Day))
```

```{r}
year18_22 <- interval(ymd("2018-01-01"), ymd("2022-12-31"))

cost18_22 <- cost[cost$Date %within% year18_22, ]
services18_22 <- services[services$Date %within% year18_22, ]
transactions18_22 <- transactions[transactions$Date %within% year18_22, ]
```



```{r}
summary(cost18_22)
```

```{r}
summary(services18_22)
```
```{r}
summary(prices)
```

```{r}
summary(transactions18_22)

```

## Aufgabe 3
```{r}
transactions_regions <- data.frame(regions=unique(transactions18_22$region))
print(transactions_regions)
```
## Aufgabe 4

```{r}
service_vendors <- services18_22[, c("vendor", "service")]
service_vendors <- unique(service_vendors)
service_vendors <- service_vendors[order(service_vendors$service),]
print(service_vendors)
```

## Aufgabe 5
```{r}
supermarkets <- transactions18_22[, c("region", "storename")]
supermarkets <- unique(supermarkets)
supermarkets <- subset(supermarkets, region %in% c('Shangh','Peking', 'Skorea'))

print(supermarkets)
```
# Marktübersicht

## Aufgabe 6

```{r}
totalsales <- aggregate(transactions18_22$Sales, by=list(transactions18_22$Product), FUN=sum)
colnames(totalsales) <- c("Group", "Total Sales")
print(totalsales)
```

```{r}
totalsales_105 <- sum(transactions18_22[transactions18_22$Product =="Gruppe105",4])

cat("Insgesamt wurden", totalsales_105, "Flaschen Limonade von Gruppe 105 verkauft.\n")
```

```{r}
total_marketsales <-sum(totalsales[totalsales$Group != "Lost Sales",2])
marketshare <- totalsales_105/total_marketsales

cat("Bei Betrachtung der tatsächlich verkauften Menge (Lost Sales ausgenommen) ergibt sich ein (gerundeter) Marktanteil von", round(marketshare, digits = 3)*100, "%. Dies entspricht fast genau dem Marktanteil aller anderen Gruppen/Produkte.")
```
 
## Aufgabe 7
```{r}
regions_105 <- subset(transactions18_22, Product == 'Gruppe105',select = c('region','storename','Sales'))
```

```{r}
regions_summary <- aggregate(regions_105$Sales, by=list(regions_105$region), FUN=sum)
colnames(regions_summary) <- c('Region', 'Total sales')

regions_summary$`Avg Sales per Supermarket` <- regions_summary$`Total sales`/5

regions_summary

```

```{r}
cat("Der Unterschied zwischen der absatzstärksten und absatzschwächsten Region beträgt",
round(1 - min(regions_summary$`Avg Sales per Supermarket`)/max(regions_summary$`Avg Sales per Supermarket`), 3)*100, "%."
)
```

## Aufgabe 8

```{r}
yearly_marketshare <- transactions18_22 %>% filter(Product != 'Lost Sales') %>% group_by(year = floor_date(Date, 'year'), Product) %>% summarise(sum_sales= sum(Sales))

marketshare105 <- yearly_marketshare %>% mutate(totalsales_year = sum(sum_sales)) %>% filter(Product=='Gruppe105') %>%  mutate(relative_marketshare = sum_sales/totalsales_year)

marketshare105

```
Der Marktanteil des Produkts 105 bleibt relativ konstant bei ungefähr 12%, dies ändert sich fast nicht über die Jahre.

## Aufgabe 9

```{r}
monthlymean_sales <- transactions18_22 %>%
  filter(Product == 'Gruppe105') %>%
  # Group and aggregate by month and year, e.g. 2019-01
  group_by(month_and_year = floor_date(Date, 'month')) %>%
  summarise(sales_per_month= sum(Sales)) %>%
  # Group and average over months in all years (e. g. January)
  group_by(month=month(month_and_year)) %>%
  summarise(mean_sales= mean(sales_per_month))

monthlymean_sales
```

```{r}
monthlymean_sales[which.max(monthlymean_sales$mean_sales),]
```
Der Monat mit dem höchsten durchschnittlichen Absatz über alle in den Daten vorhandenen Jahre von etwa 11111 Flaschen ist der November.

# Umsatz, Kosten und Profit

## Aufgabe 10
```{r}
revenue_105 <- totalsales_105*3.9
revenue_105
```
Der Gesamtumsatz im Zeitraum 2018-2022 beträgt ca. 2.41 Millionen Geldeinheiten.

```{r}
totalcost_105 <- sum(cost18_22[cost18_22$Product =="Gruppe105",2])
print(totalcost_105)
```
Die Gesamtkosten im Zeitraum 2018-2022 betragen ca. 1.95 Millionen Geldeinheiten.

```{r}
profit_105 <- revenue_105 - totalcost_105
profit_105
```
Der Gesamtprofit im Zeitraum 2018-2022 beträgt ca. 468 Tausend Geldeinheiten.

## Aufgabe 11

```{r}
# Umsatz (revenue) aggregieren
rev_data105 <- transactions18_22[, c("Product", "Sales", "Date")]
rev_data105 <- subset(rev_data105, Product %in% c('Gruppe105'))
rev_data105 <- aggregate(rev_data105$Sales, by=list(floor_date(rev_data105$Date, 'month')), FUN=sum)
rev_data105$x <- rev_data105$x*3.9

# Kosten aggregieren
cost_data105 <- subset(cost18_22, Product %in% c('Gruppe105'))
cost_data105 <- aggregate(cost_data105$Amount, by=list(floor_date(cost_data105$Date, 'month')), FUN=sum)

# Profit berechnen
profit_data105 <- merge(x = rev_data105, y = cost_data105, by = "Group.1", all = TRUE)
colnames(profit_data105) <- c('Date', 'Revenue', 'Cost')
profit_data105$Profit <- profit_data105$Revenue - profit_data105$Cost

# Nur Jahr 2020 anschauen
profit_data105 <- profit_data105[year(profit_data105$Date) == 2020,]

# Plotten
ggplot(data=profit_data105, aes(x=Date, y=Profit)) +
  geom_line(color="black")+
  geom_point()+
  geom_point(data = profit_data105[which.min(profit_data105$Profit), ], color="red", size=2) +
  geom_point(data = profit_data105[which.max(profit_data105$Profit), ], color="green", size=2)+
  scale_x_date(date_labels = "%Y %b")+
  scale_x_date(date_breaks = "1 month", date_labels = "%b %Y")+
  theme(axis.text.x=element_text(angle=60, hjust=1))
```

Im Mai 2020 wird der kleinste, im November 2020 der größte Profit erzielt.

## Aufgabe 12

```{r}
shipping_data105 <- services18_22 %>% filter(Product=="Gruppe105", service=="Shipping")

shipping_data105$late <- ifelse(shipping_data105$DaysExecuted > shipping_data105$DaysScheduled, "Verspätet", "Pünktlich")

shipping_byregion <- summarise(group_by(shipping_data105, region, late), total_cost=sum(cost))

library(scales)
ggplot(data=shipping_byregion, aes(x=region, y=total_cost, fill=late)) +
  geom_bar(position="dodge", stat="identity")+
  xlab("Region") + 
  ylab("Kosten Transportdiensleistungen")+
  scale_y_continuous(labels = unit_format(unit = "k", scale = 1e-3)) +
  theme(legend.title=element_blank()) +
  scale_fill_manual(values=c("darkolivegreen3", "brown1"))

```

Die Transportausgaben (Shipping expenses) liegen in allen Regionen für verspätete Transporte höher als für pünktliche.
Für die Region Philippinen sind nicht nur die Gesamtausgaben für Transporte am höchsten, sondern insbesondere auch für verspätete Transporte. Bei den pünktlichen Transporten liegt die Region Südkorea knapp vorne.

## Aufgabe 13

```{r}
warehouse_data <- services18_22 %>% filter(Product=="Gruppe105", service=="Warehousing" ) %>% summarise(sum_scheduled= sum(QScheduled), sum_exec=sum(QExecuted), sum_cost=sum(cost))

# Cost/ Executed Quantity
real_unitcost <- warehouse_data$sum_cost/warehouse_data$sum_exec
real_unitcost

# Cost/Scheduled Quantity
scheduled_unitcost <- warehouse_data$sum_cost / warehouse_data$sum_scheduled
scheduled_unitcost

# Comparison
real_unitcost / scheduled_unitcost - 1

```
Die Kosten für die tatsächliche Lagerleistung betragen 1,57 Geldeinheiten pro Stück, die für die geplante Lagerleistung nur 1,30 Geldeinheiten pro Stück. Damit haben wir ca. 21 % mehr gezahlt.

# Bewertung der Logistikdienstleister

## Aufgabe 14
Für eine ganzheitliche Bewertung der Qualität eines Transportdienstleisters verwenden wir das Produkt von On-Time Delivery (OTD) und In-Full Rate (IFR).
$$
KPI = IFR * OTD = \frac{\#\ of\ orders\ delivered\ complete}{\#\ of\ customer\ orders} * \frac{\#\ of\ orders\ delivered\ on\ or\ before\ due\ date}{\#\ of\ customer\ orders}
$$
Beide zusammengenommen beschreiben, wie gut wir in der Lage sind, Kundenaufträge zu erfüllen.
Wir haben uns bewusst gegen OTIF (On-Time, In-Full) als KPI entschieden, die nur Lieferungen im Zähler hätte, die sowohl vollständig als auch pünktlich sind. Die Annahme dahinter ist, dass wir auch unvollständige Lieferungen verkaufen können (wo nur einzelne Flaschen fehlen), nur eben weniger Flaschen.

```{r}
# Add new column for IFR and On-time delivery
services18_22$IFR <- services18_22$QExecuted / services18_22$QScheduled
services18_22$On_Time <- services18_22$DaysExecuted <= services18_22$DaysScheduled  # true/false

# Construct table of our vendors with quality KPI using dplyr pipe syntax
shipping_vendor_quality <- services18_22 %>%
  # Filter to only use our group's data and only look at shipping services 
  filter(Product=="Gruppe105", service=="Shipping" ) %>%
  
  # Group by Vendors
  group_by(vendor) %>%
  
  # Calculate our Vendor Quality KPI (as described above)
  # Also calculate the OTD (using the mean is sufficient because the column is TRUE (1) for on-time deliveries and FALSE (0) for late deliveries)
  # Also calculate average IFR, average cost per executed quentity and total number of shipments by that customer
 summarise(Vendor_Quality=mean(On_Time)*mean(IFR),
            OTD=mean(On_Time),
            IFR=mean(IFR),
            Avg_Cost_per_Qty=mean(cost/QExecuted),
            No_Shipments=n()) %>%  
  
  # Sort by Vendor Quality KPI
  arrange(Vendor_Quality)

shipping_vendor_quality
```

Der schlechteste Shipping-Dienstleister nach unserer Qualitätskennzahl ist hier JNT Shipping, die eine deutlich schlechtere Liefertreue bzw. OTD haben als die anderen Dienstleister. Da die IFR für Shipping-Dienstleistungen in diesem Datensatz immer 1 beträgt, ist dies der einzige Einflussfaktor. Der beste Shipping-Dienstleister, Bange+Hammer Shipping, führt für uns 690 Lieferungen aus. Bei JNT Shipping sind es nur unwesentlich weniger (660).
Neben der reinen Qualität sollten die Dienstleister auch nach ihren Kosten bewertet werden (siehe Spalte "Avg_Cost_per_Qty", also Kosten pro tatsächlich ausgeführter Einheit).

## Aufgabe 15

Wir verwenden hier analog und mit der gleichen Begründung wie bei den Transportdienstleistern eine kombinierte Rate aus OTD und IFR. Die On-Time Delivery bezieht sich hier nicht auf eine Auslieferung, sondern ob der Warehousing-Auftrag in der ursprünglich geplanten Zeit durchgeführt wurde.

```{r}
# Columns for IFR and On-time delivery --> already added above

# Construct table of our vendors with quality KPI using dplyr pipe syntax
# Analogous to above
warehouse_vendor_quality <- services18_22 %>%
  # Filter to only use our group's data and only look at Warehouse services 
  filter(Product=="Gruppe105", service=="Warehousing" ) %>%
  
  # Group by Vendors
  group_by(vendor) %>%
  
  # Calculate our Vendor Quality KPI (as described above)
  # Also calculate the OTD (using the mean is sufficient because the column is TRUE (1) for on-time deliveries and FALSE (0) for late deliveries)
  # Also calculate average IFR, average cost per executed quentity and total number of services by that customer
 summarise(Vendor_Quality=mean(On_Time)*mean(IFR),
            OTD=mean(On_Time),
            IFR=mean(IFR),
            Avg_Cost_per_Qty=mean(cost/QExecuted),
            No_WH_Services=n()) %>%  
  
  # Sort by Vendor Quality KPI
  arrange(desc(Vendor_Quality))

warehouse_vendor_quality
```

Der schlechteste Warehouse-Dienstleister nach unserer Qualitätskennzahl ist hier IntEx Warehousing, welcher die schlechsteste IFR aufweist. Da die IFR für Shipping-Dienstleistungen in diesem Datensatz immer 1 beträgt, ist dies der einzige Einflussfaktor. Insgesamt liegen die Dienstleister jedoch sehr nah beieinander (weniger als 2 Prozentpunkte Unterschied zwischen dem besten und schlechtesten). Auch die Anzahl der erbrachten Services ("No_WH_Services") ist sehr ähnlich.
Neben der reinen Qualität sollten die Dienstleister auch nach ihren Kosten bewertet werden (siehe Spalte "Avg_Cost_per_Qty", also Kosten pro tatsächlich ausgeführter Einheit).


## Aufgabe 16
```{r}
ggplot(data=warehouse_vendor_quality, aes(x=reorder(vendor, Vendor_Quality, decreasing=TRUE), y=Vendor_Quality)) +
  geom_col() +
  geom_point(aes(x=reorder(vendor, Vendor_Quality, decreasing=TRUE), y=Avg_Cost_per_Qty-1, color="Cost")) +
  xlab("Warehouse-Dienstleister") + 
  ylab("Quality") +
  theme(legend.title=element_blank()) +
  theme(axis.text.x=element_text(angle=60, hjust=1)) +
  scale_y_continuous(name="Quality", sec.axis = sec_axis(~.+1, name="Cost/Executed Qty"))
```
Insgesamt lassen sich bei den Warehouse-Dienstleistern nur wenig Unterschiede erkennen, weder bei der Qualität noch den Kosten. Eine Verbesserung der Performance scheint möglich (siehe z. B. Flying Mercury Warehousing mit vergleichsweise hoher Qualität bei niedrigen Kosten), wird aber vermutlich recht klein ausfallen. 

## Aufgabe 17
# 
```{r}
ggplot(data=shipping_vendor_quality, aes(x=reorder(vendor, Vendor_Quality, decreasing=TRUE), y=Vendor_Quality)) +
  geom_col() +
  geom_point(aes(x=reorder(vendor, Vendor_Quality, decreasing=TRUE), y=Avg_Cost_per_Qty-1, color="Cost")) +
  xlab("Shipping-Dienstleister") + 
  ylab("Quality") +
  theme(legend.title=element_blank()) +
  theme(axis.text.x=element_text(angle=60, hjust=1)) +
  scale_y_continuous(name="Quality", sec.axis = sec_axis(~.+1, name="Cost/Executed Qty"))
```
Insgesamt sehen wir bei den Shipping-Dienstleistern große Unterschiede bei der Qualität bei vergleichbaren Kosten. Besonders Bange+Hammer Shipping sticht hier mit einer vergleichsweise hohen Qualität (getrieben durch eine höhere OTD) hervor. Insgesamt ist die OTD Rate der Shipping-Dienstleister dennoch sehr niedrig.
# TODO: Plot + Kommentar
(ist z. B. Bange+Hammer Shipping in Regionen aktiv, die systematisch bessere OTD haben als andere?).

# Projektbeschreibung
Im Folgenden soll die in Aufgabe 2.1 durchgeführte Analyse in ein Projekt überführt werden, das systematisch Mehrwert für Gruppe105 generieren soll. Dazu beschreiben wir die geplante Projektdurchführung, zu verwendende Datenquellen, die Nutzer:innen der Analysen, sowie Empfehlungen aus den bisherigen Erkenntnissen.

## Projektdurchführung
Bei der Durchführung des Projektes orientieren wir uns am Standard-Vorgehen nach dem Cross-Industry Standard Process for Data Mining (CRISP-DM), der aus sechs Schritten besteht.

### 1. Problemidentifizierung
In der ersten Phase betrachten wir das wirtschaftliche Problem, noch ohne Blick auf die Daten. Dazu machen wir eine Lageanalyse, sprechen mit relevanten internen Stakeholdern wie der Logistik- oder Vertriebsabteilung, und legen abgestimmte Analyseziele fest. Diese wären zum Beispiel die Identifizierung von Möglichkeiten zur Erhöhung der Profitabilität, der Nachfrage (und damit dem Umsatz), oder der Kundenzufriedenheit.

### 2. Datenbeschaffung
Als nächstes mappen und beschreiben wir existierende Datenquellen. Basierend auf dieser Übersicht können wir relevante Daten systematisch auswählen oder wo nötig weitere Daten beschaffen.

### 3. Datenaufbereitung
Hier extrahieren wir zunächst die zuvor festgelegten Daten aus internen Quellsystemen wie SAP oder externen Datenbanken.
Im nächsten Schritt transformieren wir die Daten, indem wir ungültige Daten bereinigen, Filtern setzen (etwa nach Zeiträumen), fehlende Daten entweder ergänzen oder löschen, und Daten in das richtige Format bringen (z. B. Datumskonvertierung).
Schließlich laden wir die Daten in das ausgewählte Analyseprogramm (hier R).

### 4. Ausführen der Analyse
Basierend auf den in Schritt 1 festgelegten Zielen verfolgen wir hier verschiedene Analyserichtungen, zum Beispiel:

- Analyse der Profitabilität und möglichen Treibern, sowie Einfluss der Distribution
- Vergleich der Nachfrage zwischen und in Märkten
- Identifizierung der besten und schlechtesten Logistikdienstleister
- Analyse der Lost Sales nach Zeit, Region, und Zusammenhang zur Performance der Logistikdienstleister

In der Analyse verwenden wir eine Reihe von Methoden, um unser Analyseziel zu erreichen. Zunächst deskriptive Methoden wie Aggregationen nach verschiedenen Dimensionen wie Märkten oder Dienstleistern, dann die Berechnung verschiedener KPIs wie In-Full Rate oder On-Time Delivery Rate.
Daneben verwenden wir visuelle Methoden, um die Daten intuitiv darzustellen und leichte Analysen zu ermöglichen, z. B. Bar Charts zum Vergleich verschiedener LDL.
Darüber hinaus könnten sich je nach Projektverlauf und -zielen weiterführende Methoden aus den Bereichen Predictive/Prescriptive Analytics anbieten, wie etwa die Vorhersage der Nachfrage oder Zuverlässigkeit einzelner LDL.

### 5. Bewertung der Ergebnisse
Nach abgeschlossener Analyse bereiten wir die Ergebnisse in geeigneter Form auf, etwa in Form eines Reports mit Executive Summary. Diese präsentieren wir vor den relevanten internen Stakeholdern und diskutieren Ergebnisse und nächste Schritte. Zudem nehmen wir gemeinsam einen Abgleich mit den ursprünglichen Projektzielen vor und bewerten, ob diese erreicht wurden.

### 6. Bereitstellung und Nutzengenerierung
Im letzten Schritt geht es darum, die aus der Analyse und der Bewertung der Ergebnisse abgeleiteten Maßnahmen umzusetzen.

Weil sich das Geschäftsumfeld und damit die Datengrundlage ständig ändert, ist es erstrebenswert, von einer Einmal-Analyse wegzukommen hin zu kontinuierlichem Monitoring und Verbesserung. Dazu sollte die Analyse replizierbar gemacht werden, etwa durch Definition von festen KPIs, Erstellung von Dashboards und einer zumindest in Teilen automatisierten Dateneinspeisung.

## Datenquellen
Als Datenquelle bieten sich zunächst firmeninterne Quellsysteme wie das ERP-System an. Im SAP-Bereich sind für die gegebene Aufgabenstellung besonders die Module Sales & Distribution (SD; für Vetriebsdaten) und Finance (FI; für Kosten und Zahlungsverkauf mit LDL) relevant. Uns interessieren Tabellen wie etwa:

- VBAK und VBAP: Für Bestellungen und Bestellpositionen
- BSEG: Für Rechnungsinformationen
- LFA1: Für Informationen zu Lieferanten
- KNA1: Für Kundeninformationen

Sollte kein ERP-System verwendet werden, liegen die Daten ggf. als Mischung aus Excel-Tabellen und E-Mails mit Diensleistern vor. Diese wären in einem ersten Schritt systematisch zu erfassen und dann in eine nutzbare Form in Tabellenformat zu bringen.

Neben internen Datenquellen sind auch externe Quellen potenziell von Interesse, zum Beispiel:

- Tagesaktuelle Spotmarket-Preise für Logistikdienstleistungen
- Informationen zum Gesamtmarkt (z. B. insgesamt verkaufte Limonaden in Supermärkten, über alle Anbieter hinweg) in den gegebenen Regionen

## Nutzer:innen
Die wichtigsten Analyseergebnisse sind relevant für Führungskräfte im Bereich Supply Chain (z. B. Chief Supply Chain Officer). Daneben sind zeitnahe, relevante Ergebnisse wichtig für Angestellte, die direkt mit den LDL zusammenarbeiten bzw. diese beauftragen.

## Empfehlungen basierend auf Teil 2.1
Aus unseren Analysen in Teil 2.1 sehen wir für Logistikdienstleister folgende Schlüsse:

- Shipping-Dienstleister: Hier sollte Verträge mit Bange+Hammer Shipping priorisiert werden, da diese mit Abstand die beste OTD Performance aufzeigen, bei durchschnittlichen Kosten. Insgesamt ist jedoch eine sehr schlechte OTD Rate von unter 35% zu verzeichnen, was langfristig nicht akzeptabel ist.
- Warehousing-Dienstleister: Hier besteht kein akuter Handlungszwang. Nach Möglichkeit sollten Verträge mit Flying Mercury Warehousing ausgeweitet werden, da diese bei hoher IFR (Rang 2) die geringsten Kosten aufweisen.